package main
import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import javax.script.ScriptContext
import org.apache.spark.sql.SparkSession


object SparkSQL {
  
  case class Person (ID: Int, name: String, age: Int, numFriends: Int);
  
  def mapper (line: String): Person={
    val fields = line.split(",");
    return Person(fields(0).toInt, fields(1), fields(2).toInt, fields(3).toInt);
  }
  
  def main(args: Array[String]){
    Logger.getLogger("org").setLevel(Level.ERROR);
    
    // Session is used to have dataSet instead of rdd.
    val session = SparkSession.builder().appName("SQL").master("local[*]").getOrCreate(); // in case there is a node failure, it can get the already created session.
    
    // spark context from a sparkSession. This is required as the input csv file is not structured yet.
    val sparkContext = session.sparkContext.textFile("../fakefriends.csv");
    
    val personsrdd = sparkContext.map(mapper);
    
    // session to convert strucuted rdd to DataSet.
    import session.implicits._
    
    // This is dataSet.
    val personSet = personsrdd.toDS()
    
    // prints schema.
    personSet.printSchema()
    
    // converting it to SQL table sitting in memory inside spark. (Again this is distributed across clusters too).
    personSet.createOrReplaceTempView("PersonTable")
    
    val teenagers = session.sql("SELECT * FROM PeopleTable WHERE age > 14 AND < 19");
    
    teenagers.collect().foreach(println)
    
  }
  
}