package main

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import javax.script.ScriptContext

object CountWords {
  
  def main(args: Array[String]){
    Logger.getLogger("org").setLevel(Level.ERROR);
  
  val sc = new SparkContext("local[*]", "WordCount");
  
  val lines = sc.textFile("../book.txt");
  
  val wordsrdd = lines.flatMap(x => x.split("\\W+"));
  
  val lowercase = wordsrdd.map(x => x.toLowerCase());
  
  val popularWords = lowercase.map(x => (x, 1)).reduceByKey((x,y) => x+y);
  
  //action. This is an rdd.
  val results = popularWords.collect();
  
  val flip = results.map(x => x._2, x._1);
  
  // action. This is a scala map.
  //val counter = lowercase.countByValue();
  
  results.sorted.foreach(println);
  }
}