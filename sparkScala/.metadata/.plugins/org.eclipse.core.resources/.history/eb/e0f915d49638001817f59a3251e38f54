package main
import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import javax.script.ScriptContext
import org.apache.spark.sql.SparkSession


object SparkSQL {
  
  case class Person (ID: Int, name: String, age: Int, numFriends: Int);
  
  def mapper (line: String): Person={
    val fields = line.split(",");
    return Person(fields(0).toInt, fields(1), fields(2).toInt, fields(3).toInt);
  }
  
  def main(args: Array[String]){
    Logger.getLogger("org").setLevel(Level.ERROR);
    
    // Session is used to have dataSet instead of rdd.
    val session = SparkSession.builder().appName("SQL").master("local[*]").getOrCreate(); // in case there is a node failure, it can get the already created session.
    
    // spark context from a sparkSession. This is required as the input csv file is not structured yet.
    val sparkContext = session.sparkContext.textFile("../fakefriends.csv");
    
    val personsrdd = sparkContext.map(mapper);
    
    import session.implicits._
    
  }
  
}